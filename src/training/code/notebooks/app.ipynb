{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbd_CJ5lJQvp",
    "outputId": "0edf74e3-a632-438d-c529-89939181c0ae",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: peft in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (0.15.1)\n",
      "Requirement already satisfied: datasets in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: numba in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (0.61.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from peft) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from peft) (4.48.3)\n",
      "Requirement already satisfied: tqdm in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from peft) (1.3.0)\n",
      "Requirement already satisfied: safetensors in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from peft) (0.28.1)\n",
      "Requirement already satisfied: filelock in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from numba) (0.44.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from torch>=1.13.0->peft) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/koval/.pyenv/versions/3.13.0/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers accelerate peft bitsandbytes datasets torch mlflow -q\n",
    "#!pip install flash-attn --no-build-isolation -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQrrYR2IeZM0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "from inspect import signature\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer, recall_score, \\\n",
    "                            confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To not hang for an hour if no connection could be established\n",
    "mlflow.environment_variables.MLFLOW_HTTP_REQUEST_TIMEOUT = 10\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://nondecayed-laurinda-pleiophyllous.ngrok-free.dev\")\n",
    "\n",
    "mlflow.set_experiment(\"MLFlow and metrics interpretability testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub4ZJGgnrMrX"
   },
   "source": [
    "## Google drive mount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_unmqCYrSCs"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "oZwkz2wNeZM0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZ4yFI7kJQvs"
   },
   "outputs": [],
   "source": [
    "BASE_LOCATION: Path = Path(__file__).parent\n",
    "\n",
    "\n",
    "# Datasets\n",
    "DATASET_PATHS = {\n",
    "    \"local\": {\n",
    "        \"train\": BASE_LOCATION.parents[3] / \"datasets/data/train_set.csv\",\n",
    "        \"eval\": BASE_LOCATION.parents[3] / \"datasets/data/eval_set.csv\",\n",
    "    },\n",
    "    \"local_two\": {\"train\": \"train_set.csv\", \"eval\": \"eval_set.csv\"},\n",
    "    \"local_three\": {\n",
    "        \"train\": \"drive/MyDrive/fine_tuning/train_set.csv\",\n",
    "        \"eval\": \"drive/MyDrive/fine_tuning/eval_set.csv\",\n",
    "    },\n",
    "    \"kaggle\": {\n",
    "        \"train\": \"/kaggle/input/python-codes-time-complexity/train_set.csv\",\n",
    "        \"eval\": \"/kaggle/input/python-codes-time-complexity/eval_set.csv\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_datasets(dataset_paths=DATASET_PATHS):\n",
    "    for path in dataset_paths:\n",
    "        if os.path.exists(dataset_paths[path][\"train\"]) and os.path.exists(dataset_paths[path][\"eval\"]):\n",
    "            print(\"Data found!\")\n",
    "            return dataset_paths[path][\"train\"], dataset_paths[path][\"eval\"]\n",
    "\n",
    "    return FileNotFoundError(f\"Datasets do not exist in the current paths: {dataset_paths}\")\n",
    "\n",
    "train_set_path, eval_set_path = upload_datasets()\n",
    "\n",
    "train_set = load_dataset(\"csv\", data_files=train_set_path)[\"train\"]\n",
    "eval_set = load_dataset(\"csv\", data_files=eval_set_path)[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "H0DYqksrJQvt",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "sCgyImnMJQvt",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Hierarchy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFggTBarJQvt"
   },
   "outputs": [],
   "source": [
    "LABELS_HIERARCHY = {\n",
    "    'constant': 1,\n",
    "    'logn': 2,\n",
    "    'linear': 3,\n",
    "    'nlogn': 4,\n",
    "    'quadratic': 5,\n",
    "    'cubic': 6,\n",
    "    'np': 7\n",
    "}\n",
    "\n",
    "N_CLASSES = len(LABELS_HIERARCHY)\n",
    "\n",
    "# Hierarchy score\n",
    "def hc_score(y_true, y_pred, n_classes=N_CLASSES):\n",
    "    assert len(y_true) == len(y_pred), (\n",
    "        f\"The amount of y_true labels: {len(y_true)} does not equal to the amount of y_pred: {len(y_pred)}.\"\n",
    "    )\n",
    "\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    return (np.sum(np.abs(y_pred - y_true)) / n_classes) / n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    # Make preds & labels global for access in callbacks\n",
    "    global last_preds, last_labels\n",
    "\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits[0], axis=-1) if isinstance(logits, tuple) else np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Save for callbacking\n",
    "    last_preds, last_labels = preds, labels\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=preds)\n",
    "    \n",
    "    # Calculate F-1 Macro\n",
    "    f1_macro_score = f1_score(y_true=labels, y_pred=preds, average=\"macro\")\n",
    "\n",
    "    # Calculate per-class recall\n",
    "    recall_scores = recall_score(y_true=labels, y_pred=preds, average=None, labels=[0, 1, 4, 5, 2, 3, 6]) # reorder labels here because of how labelEncoder encodes the labels in not complexity-wise ascending order\n",
    "    recall_per_class = {}\n",
    "\n",
    "    # Zip label: score into a dict\n",
    "    for label, score in zip(LABELS_HIERARCHY.keys(), recall_scores):\n",
    "        recall_per_class[label] = np.round(score, 2)\n",
    "\n",
    "    # Calculate Hierarchy Score\n",
    "    hierarchy_score = hc_score(y_true=labels, y_pred=preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro_score,\n",
    "        \"recall_score\": recall_per_class,\n",
    "        \"hierarchy_score\": hierarchy_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval metric callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrixCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        if last_preds is not None:\n",
    "            # Calculate confusion matrix\n",
    "            disp = ConfusionMatrixDisplay.from_predictions(y_true=last_labels, y_pred=last_preds, labels=[0, 1, 4, 5, 2, 3, 6], # reorder labels here because of how labelEncoder encodes the labels in not complexity-wise ascending order\n",
    "                                                            display_labels=[\"O(1)\", \"O(logn)\", \"O(n)\",\"O(nlogn)\",\n",
    "                                                            \"O(n^2)\", \"O(n^3)\",\"np\",])\n",
    "            # Get fig and axes\n",
    "            fig = disp.figure_\n",
    "            ax = disp.ax_\n",
    "\n",
    "            # Make slightly wider to fit xtick labels\n",
    "            fig.set_size_inches(10, 5)\n",
    "            fig.tight_layout()\n",
    "            \n",
    "            #plt.show()\n",
    "\n",
    "            # Save as png and Log to MLFlow\n",
    "            fig.savefig(\"confusion_matrix.png\")\n",
    "            # Close and unregister, so that it doesn't print\n",
    "            plt.close(fig)\n",
    "            mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "\n",
    "\n",
    "class RecallScoreCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # Parse recall scores\n",
    "        recall_scores = kwargs['metrics']['eval_recall_score']\n",
    "\n",
    "        # Create a barplot\n",
    "        ax = sns.barplot(x=np.array(list(recall_scores.keys())),\n",
    "                    y=np.array(list(recall_scores.values())),\n",
    "                   )\n",
    "        # Add labels\n",
    "        ax.set_xlabel(xlabel=\"Complexity\", labelpad=20, fontsize=14)\n",
    "        ax.set_ylabel(ylabel=\"Recall score\", labelpad=20, fontsize=14)\n",
    "        \n",
    "        #plt.show()\n",
    "        \n",
    "        # Save as png and log to MLFLow\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig(\"recall_per_score.png\")\n",
    "        # Close and unregister, so that it doesn't print\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(\"recall_per_score.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Lr46o1ghJQvt",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5Hb02FFeZM0"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Setting up Label Encoder\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(train_set[\"complexity\"])\n",
    "\n",
    "def tokenize_data(data, tokenizer):\n",
    "    # Tokenizing\n",
    "    tokenized = tokenizer(\n",
    "        data[\"code\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    tokenized[\"labels\"] = labelEncoder.transform(data[\"complexity\"])\n",
    "    return tokenized\n",
    "\n",
    "def set_tokenizer(checkpoint):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint, pad_token=\"<pad>\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {checkpoint}: {e}\")\n",
    "        checkpoint = \"-\".join(checkpoint.split(\"-\")[:2])\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        print(f\"Falling back to {checkpoint}\")\n",
    "\n",
    "    X_train = train_set.map(\n",
    "        lambda x: tokenize_data(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train_set.column_names,\n",
    "    )\n",
    "    X_eval = eval_set.map(\n",
    "        lambda x: tokenize_data(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=eval_set.column_names,\n",
    "    )\n",
    "\n",
    "    # Data Collator\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    return tokenizer, data_collator, X_train, X_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading\n",
    "def set_model(checkpoint, tokenizer, ModelType=AutoModel):\n",
    "    # Setup bitsandbytes quantization config\n",
    "    \"\"\"quant_config = setup_bnb_config()\"\"\"\n",
    "\n",
    "    # Load a pretrained model\n",
    "    model = ModelType.from_pretrained(\n",
    "        checkpoint,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        num_labels=N_CLASSES,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "        #device_map=PartialState().process_index,\n",
    "        #quantization_config=quant_config,\n",
    "        #attn_implementation=\"flash_attention_2\", Only for newer models\n",
    "    )\n",
    "\n",
    "    # Accomodating the size of the token embeddings for the potential missing <pad> token\n",
    "    model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "    # Passing the pad token id to the model config\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom head for Deepseek v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom classifier head\n",
    "\"\"\"class DeepseekV2ForSequenceClassification(PreTrainedModel):\n",
    "    config_class = AutoConfig\n",
    "\n",
    "    def __init__(self, base_model, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = base_model\n",
    "\n",
    "        self.dense = nn.Linear(config.n_embd, config.num_labels, bias=False, dtype=self.model.dtype)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.config.n_embd\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, *args, **kwargs):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.dense(hidden_states)\n",
    "\n",
    "        # Batch size\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "\n",
    "        # If padding token id is not configured and the batch size is > 1\n",
    "        if self.config.pad_token_id is None and batch_size != 1:\n",
    "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
    "        # If padding token id is not configured\n",
    "        if self.config.pad_token_id is None:\n",
    "            last_non_pad_token = -1\n",
    "        # if encoded inputs exist => find the last non padded token to pool data from\n",
    "        elif input_ids is not None:\n",
    "            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, dtype=torch.int32)\n",
    "            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n",
    "            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n",
    "\n",
    "        # Pooling logits from the last non padded token across the batches\n",
    "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n",
    "\n",
    "        # Calculating loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(\n",
    "                logits=logits,\n",
    "                labels=labels,\n",
    "                pooled_logits=pooled_logits,\n",
    "                config=self.config,\n",
    "            )\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=pooled_logits)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization (BnB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitsandbytes (Quantization)\n",
    "\"\"\"def setup_bnb_config():\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "    )\n",
    "    return quant_config\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "\"\"\"peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    # target_modules = ['q_proj', 'v_proj'], # Qwen\n",
    "    target_modules=\"all-linear\",  # Heavy, universal\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",  # might not work with this on\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "XPRanUW6eZM2",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pATLtKjNJQvu",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Writing the custom metric *Hierarchy Complexity Score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3m3ZTOCqJQvv"
   },
   "outputs": [],
   "source": [
    "def hc_score(y_true, y_pred, n_classes=N_CLASSES):\n",
    "    assert len(y_true) == len(y_pred), f\"The amount of y_true labels: {len(y_true)} does not equal to the amount of y_pred: {len(y_pred)}.\"\n",
    "\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    return (np.sum(np.abs(y_pred - y_true)) / n_classes) / n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rWV3qk9VJQvv",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Computing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I5FKlpI6eZM2"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits[0], axis=-1) if isinstance(logits, tuple) else np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    # Calculate F-1 Macro\n",
    "    f1_macro_score = f1_score(labels, preds, average='macro')\n",
    "    # Calculate Hierarchy Score\n",
    "    hierarchy_score = hc_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro_score,\n",
    "        \"hierarchy_score\": hierarchy_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Nl1-kQLxJQvv",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gCsGfVZqJQvv",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Label tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4ncFFJymJQvv"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(train_set['complexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "L8pX6mfqJQvv",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feature tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YZjkk6wYeZM0"
   },
   "outputs": [],
   "source": [
    "def tokenize_data(samples, tokenizer):\n",
    "    tokenized = tokenizer(samples['code'], truncation=True, max_length=512)\n",
    "    tokenized['labels'] = labelEncoder.transform(samples['complexity'])\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def set_tokenizer(checkpoint):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint, pad_token=\"<pad>\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {checkpoint}: {e}\")\n",
    "        checkpoint = \"-\".join(checkpoint.split(\"-\")[:2])\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        print(f\"Falling back to {checkpoint}\")\n",
    "\n",
    "    X_train = train_set.map(\n",
    "        lambda x: tokenize_data(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=train_set.column_names\n",
    "    )\n",
    "    X_eval = test_set.map(\n",
    "        lambda x: tokenize_data(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=test_set.column_names\n",
    "    )\n",
    "\n",
    "    # Optional: Verify columns\n",
    "    print(\"X_train columns:\", X_train.column_names)\n",
    "    print(\"X_eval columns:\", X_eval.column_names)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    return tokenizer, data_collator, X_train, X_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tHXDPdDTcgw3",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFETQtmXb7gA"
   },
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4bI0kYYVb7gB"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zmv7vp8vJQvu",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "G4ZYhjKbJQvu"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoOUs4koZABP"
   },
   "source": [
    "## Quantizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "D4po3SB0cNy5"
   },
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9zo8AHLdHJr"
   },
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0lwbOFD1b7gA"
   },
   "outputs": [],
   "source": [
    "def set_model(checkpoint):\n",
    "    model = AutoModel.from_pretrained(checkpoint, torch_dtype='bfloat16', num_labels=7,\n",
    "                                      trust_remote_code=True, quantization_config=quant_config)\n",
    "    # Configuring padding token in case is absent\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # As well, as resizing the embeddings to accomodate the new *pad* token\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42ce8a864a3480d9e3f4550f62ea646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbfloat16\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:564\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    563\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    568\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    570\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/site-packages/transformers/modeling_utils.py:3620\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3617\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3620\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3627\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   3628\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:75\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     72\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m     )\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m     )\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype='bfloat16', num_labels=7,\n",
    "                                      trust_remote_code=True, quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxAfd25_d6Q8"
   },
   "source": [
    "## Classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "79MsuPlvb7gA"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig, PreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class DeepseekV2ForSequenceClassification(PreTrainedModel):\n",
    "    config_class = AutoConfig\n",
    "\n",
    "    def __init__(self, base_model, config):\n",
    "      super().__init__(config)\n",
    "      self.num_labels = config.num_labels\n",
    "      self.model = base_model\n",
    "\n",
    "      self.dense = nn.Linear(config.hidden_size, config.num_labels, device=self.model.device, dtype=config.torch_dtype)\n",
    "      # Initialize weights and apply final processing\n",
    "      self.post_init()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, *args, **kwargs):\n",
    "      # Filter out unexpected keyword arguments for the base model\n",
    "      model_kwargs = {k: v for k, v in kwargs.items() if k in [\n",
    "            'past_key_values', 'use_cache', 'output_attentions', \n",
    "            'output_hidden_states', 'return_dict'\n",
    "        ]}\n",
    "\n",
    "        # Call the base model with only the expected arguments\n",
    "      outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            *args,\n",
    "            **model_kwargs\n",
    "        )\n",
    "\n",
    "      hidden_states = outputs.last_hidden_state\n",
    "      logits = self.dense(hidden_states)\n",
    "\n",
    "      # Batch size\n",
    "      if input_ids is not None:\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "      # If padding token id is not configured and the batch size is > 1\n",
    "      if self.config.pad_token_id is None and batch_size != 1:\n",
    "        raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
    "      # If padding token id is not configured\n",
    "      if self.config.pad_token_id is None:\n",
    "        last_non_pad_token = -1\n",
    "      # if encoded inputs exist => find the last non padded token to pool data from\n",
    "      elif input_ids is not None:\n",
    "        non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, dtype=torch.int32)\n",
    "        token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n",
    "        last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n",
    "\n",
    "      # Pooling logits from the last non padded token across the batches\n",
    "      pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n",
    "\n",
    "      # Calculating loss if labels are provided\n",
    "      loss = None\n",
    "      if labels is not None:\n",
    "        loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n",
    "\n",
    "      return SequenceClassifierOutput(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y0BO1dIeNTS"
   },
   "source": [
    "## Loading tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MwoD-5Evb7gB"
   },
   "outputs": [],
   "source": [
    "#tokenizer, data_collator, train_set, eval_set = set_tokenizer(checkpoint)\n",
    "#base_model = set_model(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AqS4BoGELt3B"
   },
   "outputs": [],
   "source": [
    "#model = DeepseekV2ForSequenceClassification(base_model, base_model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9AdkvRfuJQvw",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "D_2-Q6JiJQvw",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Check module names in the model to specify them in *target_modules* param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7nExXbXJQvw"
   },
   "source": [
    "model = set_model(checkpoint)\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "q8vIgUyDJQvw",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "i7idu8zFJQvw"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    #r=16,\n",
    "    #lora_alpha=32,\n",
    "    #target_modules = [\"q_proj\", \"k_proj\", 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], # Not sure about this\n",
    "    target_modules = [\"q_proj\", 'v_proj'], # Not sure about this\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    #modules_to_save=['classifier'], # Not sure about this one either\n",
    "    task_type = \"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "#model = get_peft_model(model=model, peft_config=config)\n",
    "#model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "W_6R-GPqJQvw",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Flash the drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BVtDFz-yJQvw",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "!rm -rf training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pjxVL-_peZM1",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Trainer Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0_5pIM9LJQvw"
   },
   "outputs": [],
   "source": [
    "def set_training_args(checkpoint, batch_size=16):\n",
    "    training_args = TrainingArguments(output_dir=f\"training_results/{checkpoint}/\",\n",
    "                                      eval_strategy=\"epoch\",\n",
    "                                      save_strategy=\"epoch\",\n",
    "                                      logging_strategy=\"epoch\",\n",
    "                                      #learning_rate=2e-4, # Testing\n",
    "                                      bf16=True,\n",
    "                                      report_to='tensorboard',\n",
    "                                      num_train_epochs=3,\n",
    "                                      warmup_steps=100, # Testing\n",
    "                                      per_device_train_batch_size=batch_size,\n",
    "                                      per_device_eval_batch_size=batch_size,\n",
    "                                      gradient_accumulation_steps = 4,\n",
    "                                      # Testing\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      remove_unused_columns=False,\n",
    "                                     )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Q-46WHhLeZM2",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(checkpoint):\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # Set up tokenizer, datasets, and model (as before)\n",
    "    tokenizer, data_collator, train_set, eval_set = set_tokenizer(checkpoint)\n",
    "    base_model = set_model(checkpoint)\n",
    "    model = DeepseekV2ForSequenceClassification(base_model, base_model.config)\n",
    "    model = get_peft_model(model=model, peft_config=config)\n",
    "\n",
    "    # Prepare everything with Accelerator\n",
    "    model, train_set, eval_set, data_collator = accelerator.prepare(\n",
    "        model, train_set, eval_set, data_collator\n",
    "    )\n",
    "    return model, train_set, eval_set, data_collator, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "TrLddCjHJQvx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a216a48e8f49dabcc78337aea37160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train columns: ['input_ids', 'attention_mask', 'labels']\n",
      "X_eval columns: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m     trainer.save_metrics(split=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m, metrics=test_metrics)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m trainer = \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mfinetune\u001b[39m\u001b[34m(checkpoint)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfinetune\u001b[39m(checkpoint):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Setup training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     model, train_set, eval_set, data_collator, tokenizer = \u001b[43msetup_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Collecting\u001b[39;00m\n\u001b[32m      6\u001b[39m     training_args = set_training_args(checkpoint=checkpoint, batch_size=\u001b[32m16\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36msetup_training\u001b[39m\u001b[34m(checkpoint)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Set up tokenizer, datasets, and model (as before)\u001b[39;00m\n\u001b[32m      6\u001b[39m tokenizer, data_collator, train_set, eval_set = set_tokenizer(checkpoint)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m base_model = \u001b[43mset_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m model = DeepseekV2ForSequenceClassification(base_model, base_model.config)\n\u001b[32m      9\u001b[39m model = get_peft_model(model=model, peft_config=config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mset_model\u001b[39m\u001b[34m(checkpoint)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_model\u001b[39m(checkpoint):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbfloat16\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Configuring padding token in case is absent\u001b[39;00m\n\u001b[32m      5\u001b[39m     model.config.pad_token_id = tokenizer.pad_token_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:559\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    558\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    563\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/site-packages/transformers/modeling_utils.py:3620\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3617\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3620\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3627\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   3628\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.0/lib/python3.13/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:75\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     72\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m     )\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m     )\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "def finetune(checkpoint):\n",
    "    # Setup training\n",
    "    model, train_set, eval_set, data_collator, tokenizer = setup_training(checkpoint)\n",
    "\n",
    "    # Collecting\n",
    "    training_args = set_training_args(checkpoint=checkpoint, batch_size=16)\n",
    "\n",
    "    # Building\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=eval_set,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    #trainer.train()\n",
    "\n",
    "    # Save metrics\n",
    "    test_metrics = trainer.evaluate(eval_dataset=eval_set)\n",
    "    trainer.save_metrics(split=\"test\", metrics=test_metrics)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "trainer = finetune(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1EBAkLR5JQvx",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Flushing CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6fYTw2_JQvx"
   },
   "outputs": [],
   "source": [
    "!pip install GPUtil\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1tOXZ8Z_JQvx",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Drjxr-AY0Vn"
   },
   "outputs": [],
   "source": [
    "device = torch.cuda.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b67a1iLHJQvx"
   },
   "outputs": [],
   "source": [
    "tokenizer, data_collator, train_set, eval_set = set_tokenizer(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NNAeFmlJQvx"
   },
   "outputs": [],
   "source": [
    "def predict(inputs):\n",
    "    # Tokenizing inputs\n",
    "    test_sample = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = Dataset.from_dict({key: value.to(model.device) for key, value in test_sample.items()})\n",
    "\n",
    "    # Predicting & decoding inputs\n",
    "    preds = trainer.predict(test_dataset=inputs)\n",
    "    preds = labelEncoder.inverse_transform(y=np.ravel(np.argmax(preds.predictions[0], axis=-1)))\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkoEDmUMV1EU"
   },
   "outputs": [],
   "source": [
    "test_sample = \"\"\"\n",
    "class Solution:\n",
    "    def topKFrequent(self, nums: List[int], k: int) -> List[int]:\n",
    "        count = {}\n",
    "        for num in nums:\n",
    "            count[num] = 1 + count.get(num, 0)\n",
    "\n",
    "        arr = []\n",
    "        for num, cnt in count.items():\n",
    "            arr.append([cnt, num])\n",
    "        arr.sort()\n",
    "\n",
    "        res = []\n",
    "        while len(res) < k:\n",
    "            res.append(arr.pop()[1])\n",
    "        return res\n",
    "        \"\"\"\n",
    "\n",
    "predict(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iuiC1OFJQvx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6562833,
     "sourceId": 10696892,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
