# -*- coding: utf-8 -*-
"""biggitybiggityO

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/komahere/biggitybiggityo.27a543b6-3a31-4081-b678-cfc921b2b91c.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250414/auto/storage/goog4_request%26X-Goog-Date%3D20250414T224011Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da39f539b76404c3e26d6bdcac9b6f9d84fc155e35ed1c5c4cc48e1447d3f638793f648dc49a12123ad930cc470250ca013dd855e069154b6598987898cc2e1bf338cfab86e8990de951ee07cf47438ff751768b23eead77f63877bc5053f25a37ea7074f6f666f151e64b9be981ae901c407bc6e6cf5055b6d832085e791de1f404e3ecb9645d6ffcda7725054756ecafa0e5882a3d84ed198e9703bdb4949976e1aad8784505713efa96fc124f09b3e27b1e88497c0c82a9786032d8045ca0781e652724bd70c3aa0124d43993ea6b643016de6d4470d74d58c6442a87c69fa16a38023287bc098c7c2b3dee1da866429ed813aebb63639250ee7be0cfe3a56
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
komahere_python_codes_time_complexity_path = kagglehub.dataset_download('komahere/python-codes-time-complexity')

print('Data source import complete.')

# %% [code] {"id":"lbd_CJ5lJQvp","outputId":"0edf74e3-a632-438d-c529-89939181c0ae","scrolled":true,"execution":{"iopub.status.busy":"2025-04-14T21:41:07.864781Z","iopub.execute_input":"2025-04-14T21:41:07.865149Z","iopub.status.idle":"2025-04-14T21:41:11.276002Z","shell.execute_reply.started":"2025-04-14T21:41:07.865121Z","shell.execute_reply":"2025-04-14T21:41:11.274641Z"}}
pip install pandas scikit-learn peft datasets numba bitsandbytes

# %% [code] {"id":"SQrrYR2IeZM0","execution":{"iopub.status.busy":"2025-04-14T21:41:11.277472Z","iopub.execute_input":"2025-04-14T21:41:11.277767Z","iopub.status.idle":"2025-04-14T21:41:11.282854Z","shell.execute_reply.started":"2025-04-14T21:41:11.277743Z","shell.execute_reply":"2025-04-14T21:41:11.282112Z"}}
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, \
                         TrainingArguments, Trainer, DataCollatorWithPadding, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
from accelerate import Accelerator
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, make_scorer
import os
import torch
import torch.nn as nn
import pdb

# %% [markdown] {"id":"ub4ZJGgnrMrX"}
# ## Google drive mount

# %% [markdown] {"id":"m_unmqCYrSCs"}
# from google.colab import drive
# drive.mount("/content/drive")

# %% [markdown] {"id":"oZwkz2wNeZM0","jupyter":{"outputs_hidden":false}}
# # Dataset uploading

# %% [code] {"id":"rZ4yFI7kJQvs","execution":{"iopub.status.busy":"2025-04-14T21:41:11.284722Z","iopub.execute_input":"2025-04-14T21:41:11.284916Z","iopub.status.idle":"2025-04-14T21:41:11.313830Z","shell.execute_reply.started":"2025-04-14T21:41:11.284899Z","shell.execute_reply":"2025-04-14T21:41:11.313232Z"}}
DATASET_PATHS = {
    "local": {
        "train": "../../datasets/train_set.csv",
        "test": "../../datasets/test_set.csv"
    },
    "local_two": {
        "train": "train_set.csv",
        "test": "test_set.csv"
    },
    "local_three": {
        "train": "drive/MyDrive/fine_tuning/train_set.csv",
        "test": "drive/MyDrive/fine_tuning/test_set.csv"
    },

    "kaggle": {
        "train": "/kaggle/input/train_set.csv",
        "test": "/kaggle/input/test_set.csv"
    }
}

def upload_datasets(dataset_paths=DATASET_PATHS):
    for path in dataset_paths:
        if os.path.exists(dataset_paths[path]['train']) and os.path.exists(dataset_paths[path]['test']):
            return dataset_paths[path]['train'], dataset_paths[path]['test']

    return FileNotFoundError(f"Datasets do not exist in the current paths: {dataset_paths}")


train_set_path, test_set_path = upload_datasets()

# %% [markdown] {"id":"H0DYqksrJQvt","jupyter":{"outputs_hidden":false}}
# # Metrics

# %% [markdown] {"id":"sCgyImnMJQvt","jupyter":{"outputs_hidden":false}}
# ### Ordering labels by Hierarchy

# %% [code] {"id":"LFggTBarJQvt","execution":{"iopub.status.busy":"2025-04-14T21:41:11.314981Z","iopub.execute_input":"2025-04-14T21:41:11.315191Z","iopub.status.idle":"2025-04-14T21:41:11.318511Z","shell.execute_reply.started":"2025-04-14T21:41:11.315173Z","shell.execute_reply":"2025-04-14T21:41:11.317855Z"}}
LABELS_HIERARCHY = {
    'constant': 1,
    'logn': 2,
    'linear': 3,
    'nlogn': 4,
    'quadratic': 5,
    'cubic': 6,
    'np': 7
}

N_CLASSES = len(LABELS_HIERARCHY)

# %% [markdown] {"id":"Lr46o1ghJQvt","jupyter":{"outputs_hidden":false}}
# # Dataset uploading

# %% [code] {"id":"T5Hb02FFeZM0","execution":{"iopub.status.busy":"2025-04-14T21:41:11.319453Z","iopub.execute_input":"2025-04-14T21:41:11.319749Z","iopub.status.idle":"2025-04-14T21:41:11.453979Z","shell.execute_reply.started":"2025-04-14T21:41:11.319721Z","shell.execute_reply":"2025-04-14T21:41:11.453398Z"}}
train_set = load_dataset("csv", data_files=train_set_path)['train']
test_set = load_dataset("csv", data_files=test_set_path)['train']

train_labels = train_set['complexity']
test_labels = test_set['complexity']

# %% [markdown] {"id":"XPRanUW6eZM2","jupyter":{"outputs_hidden":false}}
# # Evaluating

# %% [markdown] {"id":"pATLtKjNJQvu","jupyter":{"outputs_hidden":false}}
# ### Writing the custom metric *Hierarchy Complexity Score*

# %% [code] {"id":"3m3ZTOCqJQvv","execution":{"iopub.status.busy":"2025-04-14T21:41:11.454722Z","iopub.execute_input":"2025-04-14T21:41:11.454996Z","iopub.status.idle":"2025-04-14T21:41:11.459020Z","shell.execute_reply.started":"2025-04-14T21:41:11.454968Z","shell.execute_reply":"2025-04-14T21:41:11.458314Z"}}
def hc_score(y_true, y_pred, n_classes=N_CLASSES):
    assert len(y_true) == len(y_pred), f"The amount of y_true labels: {len(y_true)} does not equal to the amount of y_pred: {len(y_pred)}."

    n_samples = len(y_true)

    return (np.sum(np.abs(y_pred - y_true)) / n_classes) / n_samples

# %% [markdown] {"id":"rWV3qk9VJQvv","jupyter":{"outputs_hidden":false}}
# ## Computing metrics

# %% [code] {"id":"I5FKlpI6eZM2","execution":{"iopub.status.busy":"2025-04-14T21:41:11.459942Z","iopub.execute_input":"2025-04-14T21:41:11.460223Z","iopub.status.idle":"2025-04-14T21:41:11.470580Z","shell.execute_reply.started":"2025-04-14T21:41:11.460187Z","shell.execute_reply":"2025-04-14T21:41:11.469878Z"}}
def compute_metrics(eval_preds):
    logits, labels = eval_preds
    preds = np.argmax(logits[0], axis=-1) if isinstance(logits, tuple) else np.argmax(logits, axis=-1)

    # Calculate accuracy
    accuracy = accuracy_score(labels, preds)
    # Calculate F-1 Macro
    f1_macro_score = f1_score(labels, preds, average='macro')
    # Calculate Hierarchy Score
    hierarchy_score = hc_score(labels, preds)

    return {
        "accuracy": accuracy,
        "f1_macro": f1_macro_score,
        "hierarchy_score": hierarchy_score
    }

# %% [markdown] {"id":"Nl1-kQLxJQvv","jupyter":{"outputs_hidden":false}}
# # Tokenizing

# %% [markdown] {"id":"gCsGfVZqJQvv","jupyter":{"outputs_hidden":false}}
# ## Label tokenizing

# %% [code] {"id":"4ncFFJymJQvv","execution":{"iopub.status.busy":"2025-04-14T21:41:11.471547Z","iopub.execute_input":"2025-04-14T21:41:11.471844Z","iopub.status.idle":"2025-04-14T21:41:11.504025Z","shell.execute_reply.started":"2025-04-14T21:41:11.471817Z","shell.execute_reply":"2025-04-14T21:41:11.503413Z"}}
labelEncoder = LabelEncoder()
labelEncoder.fit(train_set['complexity'])

# %% [markdown] {"id":"L8pX6mfqJQvv","jupyter":{"outputs_hidden":false}}
# ## Feature tokenizing

# %% [code] {"id":"YZjkk6wYeZM0","execution":{"iopub.status.busy":"2025-04-14T21:41:11.506233Z","iopub.execute_input":"2025-04-14T21:41:11.506417Z","iopub.status.idle":"2025-04-14T21:41:11.511472Z","shell.execute_reply.started":"2025-04-14T21:41:11.506401Z","shell.execute_reply":"2025-04-14T21:41:11.510742Z"}}
def tokenize_data(samples, tokenizer):
    tokenized = tokenizer(samples['code'], truncation=True, max_length=512)
    tokenized['labels'] = labelEncoder.transform(samples['complexity'])
    return tokenized


def set_tokenizer(checkpoint):
    try:
        tokenizer = AutoTokenizer.from_pretrained(checkpoint, pad_token="<pad>")
    except Exception as e:
        print(f"Failed to load {checkpoint}: {e}")
        checkpoint = "-".join(checkpoint.split("-")[:2])
        tokenizer = AutoTokenizer.from_pretrained(checkpoint)
        print(f"Falling back to {checkpoint}")

    X_train = train_set.map(
        lambda x: tokenize_data(x, tokenizer),
        batched=True,
        #remove_columns=train_set.column_names
    )
    X_eval = test_set.map(
        lambda x: tokenize_data(x, tokenizer),
        batched=True,
        #remove_columns=test_set.column_names
    )

    # Data Collator
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    return tokenizer, data_collator, X_train, X_eval

# %% [markdown] {"id":"tHXDPdDTcgw3","jupyter":{"outputs_hidden":false}}
# # Model

# %% [markdown] {"id":"GFETQtmXb7gA"}
# ## Device

# %% [code] {"id":"4bI0kYYVb7gB","execution":{"iopub.status.busy":"2025-04-14T21:41:11.512671Z","iopub.execute_input":"2025-04-14T21:41:11.512903Z","iopub.status.idle":"2025-04-14T21:41:11.522967Z","shell.execute_reply.started":"2025-04-14T21:41:11.512884Z","shell.execute_reply":"2025-04-14T21:41:11.522191Z"}}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# %% [markdown] {"id":"zmv7vp8vJQvu","jupyter":{"outputs_hidden":false}}
# ## Checkpoint

# %% [code] {"id":"G4ZYhjKbJQvu","execution":{"iopub.status.busy":"2025-04-14T21:41:11.523783Z","iopub.execute_input":"2025-04-14T21:41:11.524051Z","iopub.status.idle":"2025-04-14T21:41:11.533233Z","shell.execute_reply.started":"2025-04-14T21:41:11.524032Z","shell.execute_reply":"2025-04-14T21:41:11.532563Z"}}
checkpoint = "deepseek-ai/DeepSeek-Coder-V2-Lite-Base"
checkpoint = "gpt2"

# %% [markdown] {"id":"GoOUs4koZABP"}
# ## Quantizing

# %% [code] {"id":"D4po3SB0cNy5","execution":{"iopub.status.busy":"2025-04-14T21:41:11.534046Z","iopub.execute_input":"2025-04-14T21:41:11.534315Z","iopub.status.idle":"2025-04-14T21:41:11.546467Z","shell.execute_reply.started":"2025-04-14T21:41:11.534284Z","shell.execute_reply":"2025-04-14T21:41:11.545707Z"}}
# Configure 4-bit quantization
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_storage=torch.bfloat16
)

# %% [markdown] {"id":"i9zo8AHLdHJr"}
# ## Model loading

# %% [code] {"id":"0lwbOFD1b7gA","execution":{"iopub.status.busy":"2025-04-14T21:41:33.306343Z","iopub.execute_input":"2025-04-14T21:41:33.306670Z","iopub.status.idle":"2025-04-14T21:41:33.310770Z","shell.execute_reply.started":"2025-04-14T21:41:33.306626Z","shell.execute_reply":"2025-04-14T21:41:33.309950Z"}}
def set_model(checkpoint, tokenizer, ModelType=AutoModel):
    model = ModelType.from_pretrained(checkpoint, torch_dtype='bfloat16', num_labels=7,
                                     trust_remote_code=True, quantization_config=quant_config)
    #model = ModelType.from_pretrained(checkpoint, torch_dtype='bfloat16', num_labels=7,
     #                                trust_remote_code=True)
    # Configuring padding token in case is absent
    model.config.pad_token_id = tokenizer.pad_token_id
    # As well, as resizing the embeddings to accomodate the new *pad* token
    model.resize_token_embeddings(len(tokenizer))
    return model

# %% [markdown] {"id":"IxAfd25_d6Q8"}
# ## Classifier head

# %% [code] {"id":"79MsuPlvb7gA","execution":{"iopub.status.busy":"2025-04-14T21:41:33.327222Z","iopub.execute_input":"2025-04-14T21:41:33.327417Z","iopub.status.idle":"2025-04-14T21:41:33.336475Z","shell.execute_reply.started":"2025-04-14T21:41:33.327399Z","shell.execute_reply":"2025-04-14T21:41:33.335637Z"}}
from transformers import AutoConfig, PreTrainedModel
from transformers.modeling_outputs import SequenceClassifierOutput
import pdb

class CustomModelForSequenceClassification(PreTrainedModel):
    config_class = AutoConfig

    def __init__(self, base_model, config):
      super().__init__(config)
      self.num_labels = config.num_labels
      self.model = base_model

      self.dense = nn.Linear(config.hidden_size, config.num_labels, bias=False, device=self.model.device, dtype=self.model.dtype)

      # Initialize weights and apply final processing
      self.post_init()

    def get_input_embeddings(self):
        return self.model.hidden_size

    def forward(self, input_ids=None, attention_mask=None, labels=None, *args, **kwargs):
      # Call the base model with only the expected arguments
      outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            *args,
            **kwargs
        )

      hidden_states = outputs[0]
      logits = self.dense(hidden_states)

      # Batch size
      if input_ids is not None:
        batch_size = input_ids.shape[0]

      # If padding token id is not configured and the batch size is > 1
      if self.config.pad_token_id is None and batch_size != 1:
        raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
      # If padding token id is not configured
      if self.config.pad_token_id is None:
        last_non_pad_token = -1
      # if encoded inputs exist => find the last non padded token to pool data from
      elif input_ids is not None:
        non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, dtype=torch.int32)
        token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)
        last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)

      # Pooling logits from the last non padded token across the batches
      pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]

      # Calculating loss if labels are provided
      loss = None
      if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)

      return SequenceClassifierOutput(loss=loss, logits=pooled_logits)

# %% [markdown]
# # ---------TEST---------

# %% [markdown] {"execution":{"iopub.status.busy":"2025-04-10T19:06:20.539271Z","iopub.execute_input":"2025-04-10T19:06:20.539613Z","iopub.status.idle":"2025-04-10T19:06:20.719371Z","shell.execute_reply.started":"2025-04-10T19:06:20.539590Z","shell.execute_reply":"2025-04-10T19:06:20.718667Z"}}
# model = CustomModelForSequenceClassification(base_model, base_model.config)
# test_sample = """
# class Solution:
#     def topKFrequent(self, nums: List[int], k: int) -> List[int]:
#         count = {}
#         for num in nums:
#             count[num] = 1 + count.get(num, 0)
#
#         arr = []
#         for num, cnt in count.items():
#             arr.append([cnt, num])
#         arr.sort()
#
#         res = []
#         while len(res) < k:
#             res.append(arr.pop()[1])
#         return res
#         """
#
# inputs = tokenizer(test_sample, padding=True, truncation=True, return_tensors='pt').to(device)
# outputs = model(**inputs)

# %% [markdown] {"execution":{"iopub.status.busy":"2025-04-12T20:45:28.787458Z","iopub.execute_input":"2025-04-12T20:45:28.787742Z","iopub.status.idle":"2025-04-12T20:45:28.791318Z","shell.execute_reply.started":"2025-04-12T20:45:28.787722Z","shell.execute_reply":"2025-04-12T20:45:28.790514Z"}}
# # ---------END OF TEST---------

# %% [markdown] {"id":"4y0BO1dIeNTS"}
# ## Loading tokenizer and model

# %% [markdown] {"id":"9AdkvRfuJQvw","jupyter":{"outputs_hidden":false}}
# # LoRA

# %% [markdown] {"id":"D_2-Q6JiJQvw","jupyter":{"outputs_hidden":false}}
# #### Check module names in the model to specify them in *target_modules* param

# %% [markdown] {"id":"o7nExXbXJQvw"}
# for name, module in model.named_modules():
#     print(name)

# %% [markdown] {"id":"q8vIgUyDJQvw","jupyter":{"outputs_hidden":false}}
# ## LoRA config

# %% [code] {"id":"i7idu8zFJQvw","execution":{"iopub.status.busy":"2025-04-14T21:41:33.337499Z","iopub.execute_input":"2025-04-14T21:41:33.337817Z","iopub.status.idle":"2025-04-14T21:41:33.351309Z","shell.execute_reply.started":"2025-04-14T21:41:33.337786Z","shell.execute_reply":"2025-04-14T21:41:33.350626Z"}}
peft_config = LoraConfig(
    #r=16,
    #lora_alpha=32,
    #target_modules = ['q_proj',  'v_proj'],
    #target_modules = ['q_proj',  'o_proj'],
    lora_dropout=0.1,
    bias='none',
    #modules_to_save=['classifier'], # Not sure about this one either
    task_type = "SEQ_CLS"
)

# %% [markdown] {"id":"W_6R-GPqJQvw","jupyter":{"outputs_hidden":false}}
# ### Flash the drive

# %% [markdown] {"id":"BVtDFz-yJQvw","jupyter":{"outputs_hidden":false}}
# !rm -rf training_results

# %% [markdown] {"id":"pjxVL-_peZM1","jupyter":{"outputs_hidden":false}}
# # Trainer Args

# %% [code] {"id":"0_5pIM9LJQvw","execution":{"iopub.status.busy":"2025-04-14T21:41:33.352805Z","iopub.execute_input":"2025-04-14T21:41:33.353017Z","iopub.status.idle":"2025-04-14T21:41:33.363611Z","shell.execute_reply.started":"2025-04-14T21:41:33.352999Z","shell.execute_reply":"2025-04-14T21:41:33.362843Z"}}
def set_training_args(checkpoint, batch_size=16):
    training_args = TrainingArguments(output_dir=f"training_results/{checkpoint}/",
                                      eval_strategy="epoch",
                                      save_strategy="epoch",
                                      logging_strategy="epoch",
                                      #eval_steps=2,
                                      #learning_rate=2e-4, # Testing
                                      bf16=True,
                                      report_to='tensorboard',
                                      num_train_epochs=3,
                                      warmup_steps=100, # Testing
                                      per_device_train_batch_size=batch_size,
                                      per_device_eval_batch_size=batch_size,
                                      gradient_accumulation_steps = 4,
                                      # Testing
                                      load_best_model_at_end=True,
                                      #fsdp=["full_shard", "auto_wrap"]
                                     )
    return training_args

# %% [markdown] {"id":"Q-46WHhLeZM2","jupyter":{"outputs_hidden":false}}
# # Trainer

# %% [code] {"execution":{"iopub.status.busy":"2025-04-14T21:41:33.364472Z","iopub.execute_input":"2025-04-14T21:41:33.364760Z","iopub.status.idle":"2025-04-14T21:41:33.377959Z","shell.execute_reply.started":"2025-04-14T21:41:33.364733Z","shell.execute_reply":"2025-04-14T21:41:33.377166Z"}}
def setup_training(checkpoint):
    # Set up tokenizer, datasets, and model (as before)
    tokenizer, data_collator, train_set, eval_set = set_tokenizer(checkpoint)

    #base_model = set_model(checkpoint, tokenizer)
    base_model = set_model(checkpoint, tokenizer, AutoModelForSequenceClassification)
    #model = CustomModelForSequenceClassification(base_model, base_model.config)
    #model = get_peft_model(model=model, peft_config=peft_config)
    model = get_peft_model(model=base_model, peft_config=peft_config)

    # Output the number of trainable parameters
    model.print_trainable_parameters()

    return model, train_set, eval_set, data_collator, tokenizer

# %% [code] {"id":"TrLddCjHJQvx","execution":{"iopub.status.busy":"2025-04-14T21:41:33.393809Z","iopub.execute_input":"2025-04-14T21:41:33.394011Z","iopub.status.idle":"2025-04-14T21:41:35.069336Z","shell.execute_reply.started":"2025-04-14T21:41:33.393994Z","shell.execute_reply":"2025-04-14T21:41:35.067985Z"}}
### import torch.distributed as dist
import torch.multiprocessing as mp
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from accelerate import FullyShardedDataParallelPlugin, notebook_launcher

def finetune(checkpoint):
    # FSDP
    fsdp_plugin = FullyShardedDataParallelPlugin(auto_wrap_policy="transformer_based_wrap",
                                            cpu_ram_efficient_loading=True,
                                            sync_module_states=True,
                                            transformer_cls_names_to_wrap="GPT2Block",
                                            use_orig_params=True)

    #accelerate = Accelerator(fsdp_plugin=fsdp_plugin)

    # Set up tokenizer, datasets, and model (as before)
    model, train_set, eval_set, data_collator, tokenizer = setup_training(checkpoint)

    # Wrap in accelerator
    """model, train_set, eval_set, data_collator, tokenizer = accelerate.prepare(model,
                                                                              train_set,
                                                                              eval_set,
                                                                              data_collator,
                                                                              tokenizer
                                                                               )"""
    # Output the number of trainable parameters
    model.print_trainable_parameters()

    # Collecting
    training_args = set_training_args(checkpoint=checkpoint, batch_size=4)

    # Building
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_set,
        eval_dataset=eval_set,
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    # Train
    trainer.train()

    # Save metrics
    test_metrics = trainer.evaluate(eval_dataset=eval_set)
    trainer.save_metrics(split="test", metrics=test_metrics)

    return trainer

num_of_gpus = torch.cuda.device_count()
#trainer = finetune(checkpoint)
trainer = finetune("gpt2")

# %% [markdown] {"id":"1EBAkLR5JQvx","jupyter":{"outputs_hidden":false}}
# # Flushing CUDA

# %% [code] {"id":"q6fYTw2_JQvx","execution":{"iopub.status.busy":"2025-04-14T21:41:35.069801Z","iopub.status.idle":"2025-04-14T21:41:35.070035Z","shell.execute_reply":"2025-04-14T21:41:35.069939Z"}}
!pip install GPUtil

import torch
from GPUtil import showUtilization as gpu_usage
from numba import cuda

def free_gpu_cache():
    print("Initial GPU Usage")
    gpu_usage()

    torch.cuda.empty_cache()

    cuda.select_device(0)
    cuda.close()

free_gpu_cache()

# %% [markdown] {"id":"1tOXZ8Z_JQvx","jupyter":{"outputs_hidden":false}}
# # Inference

# %% [code] {"id":"3Drjxr-AY0Vn","execution":{"iopub.status.busy":"2025-04-14T21:41:35.070626Z","iopub.status.idle":"2025-04-14T21:41:35.070897Z","shell.execute_reply":"2025-04-14T21:41:35.070798Z"}}
device = torch.cuda.device("cuda" if torch.cuda.is_available() else "cpu")

# %% [code] {"id":"b67a1iLHJQvx","execution":{"iopub.status.busy":"2025-04-14T21:41:35.071627Z","iopub.status.idle":"2025-04-14T21:41:35.071948Z","shell.execute_reply":"2025-04-14T21:41:35.071814Z"}}
tokenizer, data_collator, train_set, eval_set = set_tokenizer(checkpoint)

# %% [code] {"execution":{"iopub.status.busy":"2025-04-14T21:41:35.073153Z","iopub.status.idle":"2025-04-14T21:41:35.073438Z","shell.execute_reply":"2025-04-14T21:41:35.073317Z"}}
model

# %% [code] {"id":"2NNAeFmlJQvx","execution":{"iopub.status.busy":"2025-04-14T21:41:35.074023Z","iopub.status.idle":"2025-04-14T21:41:35.074273Z","shell.execute_reply":"2025-04-14T21:41:35.074164Z"}}
def predict(inputs):
    # Tokenizing inputs
    test_sample = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)
    inputs = Dataset.from_dict({key: value.to(model.device) for key, value in test_sample.items()})

    # Predicting & decoding inputs
    preds = trainer.predict(test_dataset=inputs)
    preds = labelEncoder.inverse_transform(y=np.ravel(np.argmax(preds.predictions[0], axis=-1)))

    return preds

# %% [code] {"id":"dkoEDmUMV1EU","execution":{"iopub.status.busy":"2025-04-14T21:41:35.074779Z","iopub.status.idle":"2025-04-14T21:41:35.075016Z","shell.execute_reply":"2025-04-14T21:41:35.074915Z"}}
test_sample = """
class Solution:
    def topKFrequent(self, nums: List[int], k: int) -> List[int]:
        count = {}
        for num in nums:
            count[num] = 1 + count.get(num, 0)

        arr = []
        for num, cnt in count.items():
            arr.append([cnt, num])
        arr.sort()

        res = []
        while len(res) < k:
            res.append(arr.pop()[1])
        return res
        """

predict(test_sample)