{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbd_CJ5lJQvp",
        "outputId": "0edf74e3-a632-438d-c529-89939181c0ae",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.60.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m160.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install pandas scikit-learn peft datasets tensorboardX numba bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQrrYR2IeZM0"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, \\\n",
        "                         TrainingArguments, Trainer, DataCollatorWithPadding, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub4ZJGgnrMrX"
      },
      "source": [
        "## Google drive mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_unmqCYrSCs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oZwkz2wNeZM0"
      },
      "source": [
        "# Dataset uploading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ4yFI7kJQvs"
      },
      "outputs": [],
      "source": [
        "DATASET_PATHS = {\n",
        "    \"local\": {\n",
        "        \"train\": \"../../datasets/train_set.csv\",\n",
        "        \"test\": \"../../datasets/test_set.csv\"\n",
        "    },\n",
        "    \"local_two\": {\n",
        "        \"train\": \"train_set.csv\",\n",
        "        \"test\": \"test_set.csv\"\n",
        "    },\n",
        "    \"local_three\": {\n",
        "        \"train\": \"drive/MyDrive/fine_tuning/train_set.csv\",\n",
        "        \"test\": \"drive/MyDrive/fine_tuning/test_set.csv\"\n",
        "    },\n",
        "\n",
        "    \"kaggle\": {\n",
        "        \"train\": \"/kaggle/input/python-codes-time-complexity/train_set.csv\",\n",
        "        \"test\": \"/kaggle/input/python-codes-time-complexity/test_set.csv\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def upload_datasets(dataset_paths=DATASET_PATHS):\n",
        "    for path in dataset_paths:\n",
        "        if os.path.exists(dataset_paths[path]['train']) and os.path.exists(dataset_paths[path]['test']):\n",
        "            return dataset_paths[path]['train'], dataset_paths[path]['test']\n",
        "\n",
        "    return FileNotFoundError(f\"Datasets do not exist in the current paths: {dataset_paths}\")\n",
        "\n",
        "\n",
        "train_set_path, test_set_path = upload_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "H0DYqksrJQvt"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "sCgyImnMJQvt"
      },
      "source": [
        "### Ordering labels by Hierarchy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFggTBarJQvt"
      },
      "outputs": [],
      "source": [
        "LABELS_HIERARCHY = {\n",
        "    'constant': 1,\n",
        "    'logn': 2,\n",
        "    'linear': 3,\n",
        "    'nlogn': 4,\n",
        "    'quadratic': 5,\n",
        "    'cubic': 6,\n",
        "    'np': 7\n",
        "}\n",
        "\n",
        "N_CLASSES = len(LABELS_HIERARCHY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Lr46o1ghJQvt"
      },
      "source": [
        "# Dataset uploading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5Hb02FFeZM0"
      },
      "outputs": [],
      "source": [
        "train_set = load_dataset(\"csv\", data_files=train_set_path)['train']\n",
        "test_set = load_dataset(\"csv\", data_files=test_set_path)['train']\n",
        "\n",
        "train_labels = train_set['complexity']\n",
        "test_labels = test_set['complexity']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XPRanUW6eZM2"
      },
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pATLtKjNJQvu"
      },
      "source": [
        "### Writing the custom metric *Hierarchy Complexity Score*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m3ZTOCqJQvv"
      },
      "outputs": [],
      "source": [
        "def hc_score(y_true, y_pred, n_classes=N_CLASSES):\n",
        "    assert len(y_true) == len(y_pred), f\"The amount of y_true labels: {len(y_true)} does not equal to the amount of y_pred: {len(y_pred)}.\"\n",
        "\n",
        "    n_samples = len(y_true)\n",
        "\n",
        "    return (np.sum(np.abs(y_pred - y_true)) / n_classes) / n_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "rWV3qk9VJQvv"
      },
      "source": [
        "## Computing metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5FKlpI6eZM2"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    preds = np.argmax(logits[0], axis=-1) if isinstance(logits, tuple) else np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    # Calculate F-1 Macro\n",
        "    f1_macro_score = f1_score(labels, preds, average='macro')\n",
        "    # Calculate Hierarchy Score\n",
        "    hierarchy_score = hc_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_macro\": f1_macro_score,\n",
        "        \"hierarchy_score\": hierarchy_score\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Nl1-kQLxJQvv"
      },
      "source": [
        "# Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gCsGfVZqJQvv"
      },
      "source": [
        "## Label tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ncFFJymJQvv"
      },
      "outputs": [],
      "source": [
        "labelEncoder = LabelEncoder()\n",
        "labelEncoder.fit(train_set['complexity'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "L8pX6mfqJQvv"
      },
      "source": [
        "## Feature tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZjkk6wYeZM0"
      },
      "outputs": [],
      "source": [
        "def tokenize_data(samples, tokenizer):\n",
        "    tokenized = tokenizer(samples['code'], truncation=True, max_length=512)\n",
        "    tokenized['labels'] = labelEncoder.transform(samples['complexity'])\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "def set_tokenizer(checkpoint):\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(checkpoint, pad_token = \"<pad>\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {checkpoint}: {e}\")\n",
        "        checkpoint = \"-\".join(checkpoint.split(\"-\")[:2])\n",
        "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "        print(f\"Falling back to {checkpoint}\")\n",
        "\n",
        "    X_train = train_set.map(lambda x: tokenize_data(x, tokenizer), batched=True)\n",
        "    X_eval = test_set.map(lambda x: tokenize_data(x, tokenizer), batched=True)\n",
        "\n",
        "    # Collator for batch padding\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    return tokenizer, data_collator, X_train, X_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tHXDPdDTcgw3"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFETQtmXb7gA"
      },
      "source": [
        "## Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bI0kYYVb7gB"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zmv7vp8vJQvu"
      },
      "source": [
        "## Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4ZYhjKbJQvu"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoOUs4koZABP"
      },
      "source": [
        "## Quantizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4po3SB0cNy5"
      },
      "outputs": [],
      "source": [
        "# Configure 4-bit quantization\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type = 'nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9zo8AHLdHJr"
      },
      "source": [
        "## Model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lwbOFD1b7gA"
      },
      "outputs": [],
      "source": [
        "def set_model(checkpoint):\n",
        "    model = AutoModel.from_pretrained(checkpoint, torch_dtype='bfloat16', num_labels=7,\n",
        "                                      trust_remote_code=True, device_map='auto', quantization_config=quant_config)\n",
        "    # Configuring padding token in case is absent\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    # As well, as resizing the embeddings to accomodate the new *pad* token\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxAfd25_d6Q8"
      },
      "source": [
        "## Classifier head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79MsuPlvb7gA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoConfig, PreTrainedModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "class DeepseekV2ForSequenceClassification(PreTrainedModel):\n",
        "    config_class = AutoConfig\n",
        "\n",
        "    def __init__(self, base_model, config):\n",
        "      super().__init__(config)\n",
        "      self.num_labels = config.num_labels\n",
        "      self.model = base_model\n",
        "\n",
        "      self.dense = nn.Linear(config.hidden_size, config.num_labels, device=device, dtype=config.torch_dtype)\n",
        "      # Initialize weights and apply final processing\n",
        "      self.post_init()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None, *args, **kwargs):\n",
        "      outputs = self.model(input_ids, attention_mask)\n",
        "\n",
        "      hidden_states = outputs.last_hidden_state\n",
        "      logits = self.dense(hidden_states)\n",
        "\n",
        "      # Batch size\n",
        "      if input_ids is not None:\n",
        "        batch_size = input_ids.shape[0]\n",
        "\n",
        "      # If padding token id is not configured and the batch size is > 1\n",
        "      if self.config.pad_token_id is None and batch_size != 1:\n",
        "        raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
        "      # If padding token id is not configured\n",
        "      if self.config.pad_token_id is None:\n",
        "        last_non_pad_token = -1\n",
        "      # if encoded inputs exist => find the last non padded token to pool data from\n",
        "      elif input_ids is not None:\n",
        "        non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, dtype=torch.int32)\n",
        "        token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n",
        "        last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n",
        "\n",
        "      # Pooling logits from the last non padded token across the batches\n",
        "      pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n",
        "\n",
        "      # Calculating loss if labels are provided\n",
        "      loss = None\n",
        "      if labels is not None:\n",
        "        loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n",
        "\n",
        "      return SequenceClassifierOutput(loss=loss, logits=logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y0BO1dIeNTS"
      },
      "source": [
        "## Loading tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwoD-5Evb7gB"
      },
      "outputs": [],
      "source": [
        "tokenizer, data_collator, train_set, eval_set = set_tokenizer(checkpoint)\n",
        "base_model = set_model(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqS4BoGELt3B"
      },
      "outputs": [],
      "source": [
        "model = DeepseekV2ForSequenceClassification(base_model, base_model.config)\n",
        "\n",
        "foo = \"Hello World\"\n",
        "inputs = tokenizer(foo, return_tensors='pt').to(device)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9AdkvRfuJQvw"
      },
      "source": [
        "# LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "D_2-Q6JiJQvw"
      },
      "source": [
        "#### Check module names in the model to specify them in *target_modules* param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7nExXbXJQvw"
      },
      "source": [
        "model = set_model(checkpoint)\n",
        "for name, module in model.named_modules():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "q8vIgUyDJQvw"
      },
      "source": [
        "## LoRA config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7idu8zFJQvw"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    #r=16,\n",
        "    #lora_alpha=32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], # Not sure about this\n",
        "    lora_dropout=0.1,\n",
        "    bias='none',\n",
        "    #modules_to_save=['classifier'], # Not sure about this one either\n",
        "    task_type = \"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model=model, peft_config=config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "W_6R-GPqJQvw"
      },
      "source": [
        "### Flash the drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BVtDFz-yJQvw"
      },
      "source": [
        "!rm -rf training_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pjxVL-_peZM1"
      },
      "source": [
        "# Trainer Args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_5pIM9LJQvw"
      },
      "outputs": [],
      "source": [
        "def set_training_args(checkpoint, batch_size=16):\n",
        "    training_args = TrainingArguments(output_dir=f\"training_results/{checkpoint}/\",\n",
        "                                      eval_strategy=\"epoch\",\n",
        "                                      save_strategy=\"epoch\",\n",
        "                                      logging_strategy=\"epoch\",\n",
        "                                      #learning_rate=2e-4, # Testing\n",
        "                                      bf16=True,\n",
        "                                      report_to='tensorboard',\n",
        "                                      num_train_epochs=3,\n",
        "                                      warmup_steps=100, # Testing\n",
        "                                      per_device_train_batch_size=batch_size,\n",
        "                                      per_device_eval_batch_size=batch_size,\n",
        "                                      gradient_accumulation_steps = 8,\n",
        "                                      # Testing\n",
        "                                      load_best_model_at_end=True,\n",
        "                                     )\n",
        "    return training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Q-46WHhLeZM2"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrLddCjHJQvx"
      },
      "outputs": [],
      "source": [
        "def finetune(checkpoint):\n",
        "    # Collecting\n",
        "    training_args = set_training_args(checkpoint=checkpoint, batch_size=2)\n",
        "\n",
        "    # Building\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_set,\n",
        "        eval_dataset=eval_set,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Save metrics\n",
        "    test_metrics = trainer.evaluate(eval_dataset=eval_set)\n",
        "    trainer.save_metrics(split=\"test\", metrics=test_metrics)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "trainer = finetune(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1EBAkLR5JQvx"
      },
      "source": [
        "# Flushing CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6fYTw2_JQvx"
      },
      "outputs": [],
      "source": [
        "!pip install GPUtil\n",
        "\n",
        "import torch\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "from numba import cuda\n",
        "\n",
        "def free_gpu_cache():\n",
        "    print(\"Initial GPU Usage\")\n",
        "    gpu_usage()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "    cuda.select_device(0)\n",
        "\n",
        "free_gpu_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1tOXZ8Z_JQvx"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Drjxr-AY0Vn"
      },
      "outputs": [],
      "source": [
        "device = torch.cuda.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b67a1iLHJQvx"
      },
      "outputs": [],
      "source": [
        "tokenizer, data_collator, train_set, eval_set = set_tokenizer(checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NNAeFmlJQvx"
      },
      "outputs": [],
      "source": [
        "def predict(inputs):\n",
        "    # Tokenizing inputs\n",
        "    test_sample = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)\n",
        "    inputs = Dataset.from_dict({key: value.to(model.device) for key, value in test_sample.items()})\n",
        "\n",
        "    # Predicting & decoding inputs\n",
        "    preds = trainer.predict(test_dataset=inputs)\n",
        "    preds = labelEncoder.inverse_transform(y=np.ravel(np.argmax(preds.predictions[0], axis=-1)))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkoEDmUMV1EU"
      },
      "outputs": [],
      "source": [
        "test_sample = \"\"\"\n",
        "class Solution:\n",
        "    def topKFrequent(self, nums: List[int], k: int) -> List[int]:\n",
        "        count = {}\n",
        "        for num in nums:\n",
        "            count[num] = 1 + count.get(num, 0)\n",
        "\n",
        "        arr = []\n",
        "        for num, cnt in count.items():\n",
        "            arr.append([cnt, num])\n",
        "        arr.sort()\n",
        "\n",
        "        res = []\n",
        "        while len(res) < k:\n",
        "            res.append(arr.pop()[1])\n",
        "        return res\n",
        "        \"\"\"\n",
        "\n",
        "predict(test_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iuiC1OFJQvx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6562833,
          "sourceId": 10696892,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30839,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
